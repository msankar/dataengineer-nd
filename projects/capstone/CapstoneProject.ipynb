{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration trends\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this capstone project, I used immigration dataset from 2016 to study immigration patterns into the US and answer following questions\n",
    " * where do immigrants come from \n",
    " * what cities do immigrants go\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import configparser\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s  [%(name)s] %(message)s')\n",
    "LOG = logging.getLogger('immig_etl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Immigration dataset from 2016 and city demographics dataset is used to study immigration patterns into the US. Used immigration data from US National Tourism and Trade office and City demographic data to understand these trends.  Used Pandas library to read and load data into Redshift. SQL and python scripts were created to build the ETL.\n",
    "\n",
    "#### Data source used and description\n",
    "* Immigration data - This data comes from the US National Tourism and Trade Office. https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "* US City demographic data - Data source is opensoft. https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "* Airport code - Table of airport codes and cities. https://datahub.io/core/airport-codes#data \n",
    "* I94_SAS_Labels_Descriptions - Sas file provided gives a better understanding of the data in immigration dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data description and data gathering\n",
    "Before loading the data into a Redshift db, I will do some exploratory data analysis here to find out what cleaning steps must be performed and understand the DDL I must define for my dim and fact tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Data exploration \n",
    "## Immigration data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data_fnames = ['../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>346608285.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>346627585.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381092385.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381087885.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381078685.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    7.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "1    8.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "2    9.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "3   10.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "4   11.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...         NaN      NaN   1996.0       D/S      M    NaN   \n",
       "1      NaN   ...         NaN      NaN   1996.0       D/S      M    NaN   \n",
       "2  20480.0   ...         NaN        M   1999.0  07152016      F    NaN   \n",
       "3  20499.0   ...         NaN        M   1971.0  07152016      F    NaN   \n",
       "4  20499.0   ...         NaN        M   2004.0  07152016      M    NaN   \n",
       "\n",
       "  airline       admnum fltno visatype  \n",
       "0      LH  346608285.0   424       F1  \n",
       "1      LH  346627585.0   424       F1  \n",
       "2      AF  381092385.0   338       B2  \n",
       "3      AF  381087885.0   338       B2  \n",
       "4      AF  381078685.0   338       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename = immigration_data_fnames[0]\n",
    "iterator = pd.read_sas(\n",
    "    filename, 'sas7bdat', encoding='ISO-8859-1', chunksize=20\n",
    ")\n",
    "idf = next(iterator)\n",
    "print(idf.shape)\n",
    "idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for fname in immigration_data_fnames:\n",
    "    files = pd.read_sas(fname, 'sas7bdat', encoding='ISO-8859-1', chunksize=20)\n",
    "    dfs.append(next(files))\n",
    "    continue\n",
    "# print(dfs[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/* I94YR - 4 digit year */\\n', '/* I94MON - Numeric month */\\n', '/* I94CIT & I94RES - This format shows all the valid and invalid codes for processing */\\n', '/* I94PORT - This format shows all the valid and invalid codes for processing */\\n', '/* I94MODE - There are missing values as well as not reported (9) */\\n', '/* I94BIR - Age of Respondent in Years */\\n', '/* COUNT - Used for summary statistics */\\n', '/* DTADFILE - Character Date Field - Date added to I-94 Files - CIC does not use */\\n', '/* VISAPOST - Department of State where where Visa was issued - CIC does not use */\\n', '/* OCCUP - Occupation that will be performed in U.S. - CIC does not use */\\n', '/* ENTDEPA - Arrival Flag - admitted or paroled into the U.S. - CIC does not use */\\n', '/* ENTDEPD - Departure Flag - Departed, lost I-94 or is deceased - CIC does not use */\\n', '/* ENTDEPU - Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use */\\n', '/* MATFLAG - Match flag - Match of arrival and departure records */\\n', '/* BIRYEAR - 4 digit year of birth */\\n', '/* DTADDTO - Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use */\\n', '/* GENDER - Non-immigrant sex */\\n', '/* INSNUM - INS number */\\n', '/* AIRLINE - Airline used to arrive in U.S. */\\n', '/* ADMNUM - Admission Number */\\n', '/* FLTNO - Flight number of Airline used to arrive in U.S. */\\n', '/* VISATYPE - Class of admission legally admitting the non-immigrant to temporarily stay in U.S. */\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('data/I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    #readFile = f.read()\n",
    "    #print(readFile)\n",
    "    f.seek(0)\n",
    "    lines = f.readlines()\n",
    "comments = [line for line in lines if line.startswith('/*') and line.endswith('*/\\n')]\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Column descriptions\n",
    "Understanding the columns in Immigration dataset.\n",
    "* cicid - primary key\n",
    "* i94yr - arrival year\n",
    "* i94mon - arrival month\n",
    "* i94cit - citizenship\n",
    "* i94res - country of residence\n",
    "* i94port - arrival port\n",
    "* arrdate - arrival date\n",
    "* i94mode - mode of arrival land, sea, air\n",
    "* i94addr - arrival address\n",
    "* depdate - departure date\n",
    "* i94bir - age of immigrant in years\n",
    "* i94visa - visa type: business, pleasure, student\n",
    "* count - \"used for summary statistics\" \n",
    "* dtadfile - character date field CIC does not use\n",
    "* visapost - Department of State where visa was issued - CIC does not use\n",
    "* occup - occupation that will be performed. CIC does not use\n",
    "* entdepd - Arrival Flag - admitted or paroled into the U.S. - CIC does not use\n",
    "* entdepu - Departure Flag - Departed, lost I-94 or is deceased - CIC does not use\n",
    "* matflag - match of arrival and departure records\n",
    "* biryear - year of birth\n",
    "* dtaddto - max date of stay - CIC does not use\n",
    "* gender - gender\n",
    "* insnum - INS number\n",
    "* airline - airline of arrival\n",
    "* admnum - admission number\n",
    "* fltno - flight number of arrival flight\n",
    "* visatype - class of admission legally admitting non-immigrant to temporarily stay in US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "zipped = zip(immigration_data_fnames, dfs)\n",
    "# list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cnames_month = {z[0].split('/')[-1].split('_')[1][:3]: list(z[1].columns.values) for z in zipped}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "jun\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(cnames_month['jan']))\n",
    "for k, v in cnames_month.items():\n",
    "    if (len(cnames_month[k]) != 28):\n",
    "        print(k)\n",
    "        #print(v)\n",
    "        print(len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Month of June has more 6 olumns than other months.\n",
    "#### Cleaning Steps\n",
    "Let's identify the columns that are not in other months and clean up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cicid', 'cicid')\n",
      "('i94yr', 'i94yr')\n",
      "('i94mon', 'i94mon')\n",
      "('i94cit', 'i94cit')\n",
      "('i94res', 'i94res')\n",
      "('i94port', 'i94port')\n",
      "('arrdate', 'arrdate')\n",
      "('i94mode', 'i94mode')\n",
      "('i94addr', 'i94addr')\n",
      "('depdate', 'depdate')\n",
      "('i94bir', 'i94bir')\n",
      "('i94visa', 'i94visa')\n",
      "('count', 'count')\n",
      "('dtadfile', 'validres')\n",
      "('visapost', 'delete_days')\n",
      "('occup', 'delete_mexl')\n",
      "('entdepa', 'delete_dup')\n",
      "('entdepd', 'delete_visa')\n",
      "('entdepu', 'delete_recdup')\n",
      "('matflag', 'dtadfile')\n",
      "('biryear', 'visapost')\n",
      "('dtaddto', 'occup')\n",
      "('gender', 'entdepa')\n",
      "('insnum', 'entdepd')\n",
      "('airline', 'entdepu')\n",
      "('admnum', 'matflag')\n",
      "('fltno', 'biryear')\n",
      "('visatype', 'dtaddto')\n"
     ]
    }
   ],
   "source": [
    "jan = list(dfs[0].columns.values)\n",
    "jun = list(dfs[5].columns.values)\n",
    "\n",
    "for z in (zip(jan, jun)):\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see that June has 'validres' and 5 other columns that begin with delete_. When immigration data is loaded into fact tables skip these 6 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Temperature data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8599212, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "print(temperature_df.shape)\n",
    "temperature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## City demographics data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_demo_df = pd.read_csv('data/us-cities-demographics.csv', delimiter=';')\n",
    "print(city_demo_df.shape)\n",
    "city_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's look at one city Newark to understand data partition. We see that eatch row is at City/State/Race level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>144961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Asian</td>\n",
       "      <td>7349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>100432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City       State  Median Age  Male Population  Female Population  \\\n",
       "4     Newark  New Jersey        34.6         138040.0           143873.0   \n",
       "1770  Newark  New Jersey        34.6         138040.0           143873.0   \n",
       "1967  Newark  New Jersey        34.6         138040.0           143873.0   \n",
       "2168  Newark  New Jersey        34.6         138040.0           143873.0   \n",
       "2580  Newark  New Jersey        34.6         138040.0           143873.0   \n",
       "\n",
       "      Total Population  Number of Veterans  Foreign-born  \\\n",
       "4               281913              5829.0       86253.0   \n",
       "1770            281913              5829.0       86253.0   \n",
       "1967            281913              5829.0       86253.0   \n",
       "2168            281913              5829.0       86253.0   \n",
       "2580            281913              5829.0       86253.0   \n",
       "\n",
       "      Average Household Size State Code                               Race  \\\n",
       "4                       2.73         NJ                              White   \n",
       "1770                    2.73         NJ          Black or African-American   \n",
       "1967                    2.73         NJ                              Asian   \n",
       "2168                    2.73         NJ  American Indian and Alaska Native   \n",
       "2580                    2.73         NJ                 Hispanic or Latino   \n",
       "\n",
       "       Count  \n",
       "4      76402  \n",
       "1770  144961  \n",
       "1967    7349  \n",
       "2168    2268  \n",
       "2580  100432  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_demo_df[city_demo_df['City'] == 'Newark'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's check if city data needs cleaning up. Things to check include\n",
    "* Male and Female population adds up to Total population in this dataset.\n",
    "* Foreign born, Number of verterans populations are less than total population.\n",
    "* Distinct states should be close to 50. DC and Puerto Rico are the other entities besides states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Confirm city data looks clean.\n",
    "for _, row in city_demo_df.iterrows():\n",
    "    if pd.notnull(row['Foreign-born']):\n",
    "        assert row['Foreign-born'] <= row['Total Population']\n",
    "    if pd.notnull(row['Male Population']):\n",
    "        assert row['Male Population'] + row['Female Population'] == row['Total Population']\n",
    "    if pd.notnull(row['Number of Veterans']):\n",
    "        assert row['Number of Veterans'] <= row['Total Population']\n",
    "    if pd.notnull(row['Count']):\n",
    "        assert row['Count'] <= row['Total Population']\n",
    "    for coll in ['Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Count']:\n",
    "        assert pd.isnull(row[coll]) or row[coll] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "City dataset looks clean. Let's look at state data and do some sanity tests. We should expect CA, TX should be in the top 5 most populated states. The number of states should be at most 50 + DC and PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State Code</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Foreign-born</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>61055672.0</td>\n",
       "      <td>62388681.0</td>\n",
       "      <td>123444353</td>\n",
       "      <td>37059662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TX</td>\n",
       "      <td>34862194.0</td>\n",
       "      <td>35691659.0</td>\n",
       "      <td>70553853</td>\n",
       "      <td>14498054.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NY</td>\n",
       "      <td>23422799.0</td>\n",
       "      <td>25579256.0</td>\n",
       "      <td>49002055</td>\n",
       "      <td>17186873.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FL</td>\n",
       "      <td>15461937.0</td>\n",
       "      <td>16626425.0</td>\n",
       "      <td>32306132</td>\n",
       "      <td>7845566.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IL</td>\n",
       "      <td>10943864.0</td>\n",
       "      <td>11570526.0</td>\n",
       "      <td>22514390</td>\n",
       "      <td>4632600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State Code  Male Population  Female Population  Total Population  \\\n",
       "4          CA       61055672.0         62388681.0         123444353   \n",
       "44         TX       34862194.0         35691659.0          70553853   \n",
       "34         NY       23422799.0         25579256.0          49002055   \n",
       "9          FL       15461937.0         16626425.0          32306132   \n",
       "14         IL       10943864.0         11570526.0          22514390   \n",
       "\n",
       "    Foreign-born  \n",
       "4     37059662.0  \n",
       "44    14498054.0  \n",
       "34    17186873.0  \n",
       "9      7845566.0  \n",
       "14     4632600.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_df = city_demo_df[['State Code', 'Male Population', 'Female Population', 'Total Population', 'Foreign-born']]\n",
    "state_df = state_df.groupby(['State Code']).sum().reset_index()\n",
    "print(state_df.shape)\n",
    "state_df.sort_values(by=['Total Population'], ascending=False).head() #we have 49 rows and CA, TX and NY are in top 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Airport data exploration\n",
    "To understand airport data\n",
    "* Check the data size.\n",
    "* Number of foreign airports.\n",
    "* Do we have airports with coordinates that is NULL.\n",
    "* Find out if all continent lengths are 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55075, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df = pd.read_csv('data/airport-codes_csv.csv')\n",
    "print(airport_codes_df.shape)\n",
    "airport_codes_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32318, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count foreign airports\n",
    "airport_codes_df[airport_codes_df['iso_country'] != 'US'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ident, type, name, elevation_ft, continent, iso_country, iso_region, municipality, gps_code, iata_code, local_code, coordinates]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we have any coordinates that is NULL\n",
    "airport_codes_df['coordinates'].isnull().values.any()\n",
    "# Are all continents length 2 ?\n",
    "airport_codes_df[airport_codes_df['continent'].str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Since the scope of the project is to understand immigration events, our data model will be a star schema with immigration data as the fact table.  Immigration data will be loaded into a table, data will be explored, validated and cleaned after it is loaded. We will then create several dimension tables around our fact immigration data which will be detailed below. To ensure my data model is reproducible I will be creating fact and dim tables in python code. See following python files in this workspace.\n",
    "* sql_queries.py - has data model of fact and dim tables.\n",
    "* create_tables.py - Creates dim and fact tables in Redshift.\n",
    "* etl.py - Builds the pipeline to load the data.\n",
    "\n",
    "First, here is the data model for immigration data. Mirrors the columns in the immigration data set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS fact_immigration \n",
    "(immigration_id INTEGER IDENTITY(0,1) PRIMARY KEY,\n",
    " cicid INT NOT NULL,\n",
    " i94yr INT NOT NULL,\n",
    " i94mon INT NOT NULL,\n",
    " i94cit INT,\n",
    " i94res INT,\n",
    " i94port CHAR(5),\n",
    " arrdate INT,\n",
    " i94mode INT,\n",
    " i94addr CHAR(5),\n",
    " depdate INT,\n",
    " i94bir INT,\n",
    " i94visa INT,\n",
    " count INT,\n",
    " dtadfile VARCHAR,\n",
    " visapost CHAR(5),\n",
    " occup CHAR(5),\n",
    " entdepa CHAR(1),\n",
    " entdepd CHAR(1),\n",
    " entdepu CHAR(1),\n",
    " matflag CHAR(1),\n",
    " biryear INT,\n",
    " dtaddto VARCHAR,\n",
    " gender CHAR(1),\n",
    " insnum VARCHAR,\n",
    " airline CHAR(3),\n",
    " admnum VARCHAR,\n",
    " fltno VARCHAR,\n",
    " visatype CHAR(3)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension table _**d_arrivalmode**_\n",
    "* Foreign key is fact_immigration.i94mode"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS d_arrivalmode (id INT, mode CHAR(12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension table _**d_visatype**_\n",
    "* Foreign key is fact_immigration.i94visa"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS d_visatype (id INT, visa_type CHAR(8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension table _**d_address**_\n",
    "* Foreign key is fact_immigration.i94addr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS d_address (id CHAR(5), name VARCHAR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension table _**d_port**_\n",
    "\n",
    "Foreign key is fact_immigration.i94port"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS d_port (id CHAR(5), name VARCHAR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension table _**d_country**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS d_country(id INT PRIMARY KEY, name VARCHAR NOT NULL);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension table _**d_city**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS d_city\n",
    "(city VARCHAR, state VARCHAR, median_age numeric, male_pop INT, female_pop INT, total_pop INT, num_vets INT,\n",
    "foreign_born INT, avg_household_size FLOAT, state_code CHAR(2), race VARCHAR, count INT);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension table _**d_state**_\n",
    "* Data aggregated from city table explained in data exploration."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS d_state\n",
    "(state_code CHAR(2) PRIMARY KEY, male_pop INT, female_pop INT, total_pop INT, foreign_born INT);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Following steps will be performed to map out data pipelines. All data will be imported from python file **etl.py** in this workspace. Reason being we want this etl load process to be reproducible and re-runnable.\n",
    "* Manually insert dimenstion arrival modes. This has 3 distinct modes.\n",
    "* Manually insert dimension visa type. This has 4 distinct visa types.\n",
    "* Populate dimension address by parsing out lines from header file.\n",
    "* Populate dimension port table by parsing out lines from header file.\n",
    "* Populate dimension country table by parsing out lines from header file.\n",
    "* Populate dimension city table by reading data from us-cities-demographics.csv\n",
    "* Populate dimension state table by simply aggregating state data from us-cities-demographics.csv\n",
    "* Populate immigration fact table by reading 12 files from immigration data frame. Here we have to be careful importing June data and not import the 6 columns we identified in out data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model. See **sql_queries.py** and **create_tables.py** for dropping and creating dimension and fact tables.\n",
    "Run the following command to create table from terminal window. _**python create_tables.py**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create the data pipeline from python scripts\n",
    "Run the following python scripts to create the pipeline. I used jupyter notebook to design and check the scripts, but highly recommend running python scripts to re-create the pipeline.\n",
    "\n",
    "* Create a REDSHIFT instance in AWS, Use StartupRedshift.ipynb as a template. Make sure your settings are cofigured in dwh.cfg\n",
    "* _**python create_tables.py**_\n",
    "* _**python etl.py**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import create_dim_table_queries, drop_dim_table_queries, create_fact_table_queries, drop_fact_table_queries\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SQL queries to drop dimension and fact tables.\n",
    "drop_arrivalmode = 'DROP TABLE IF EXISTS d_arrivalmode;'\n",
    "drop_visatype = 'DROP TABLE IF EXISTS d_visatype;'\n",
    "drop_date = 'DROP TABLE IF EXISTS d_date;'\n",
    "drop_address = 'DROP TABLE IF EXISTS d_address;'\n",
    "drop_port = 'DROP TABLE IF EXISTS d_port;'\n",
    "drop_country = 'DROP TABLE IF EXISTS d_country;'\n",
    "drop_city = 'DROP TABLE IF EXISTS d_city;'\n",
    "drop_state = 'DROP TABLE IF EXISTS d_state;'\n",
    "drop_immig_fact = 'DROP TABLE IF EXISTS fact_immigration;'\n",
    "drop_temp = 'DROP TABLE IF EXISTS d_temperature;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop all dimension tables\n",
    "drop_dim_table_queries = [drop_arrivalmode, drop_visatype, drop_date, drop_address, drop_port, drop_country, drop_city, drop_state, drop_temp]\n",
    "for query in drop_dim_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop fact table\n",
    "drop_fact_table_queries = [drop_immig_fact ]\n",
    "for query in drop_fact_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CREATE DIMENSION and FACT tables\n",
    "create_arrivalmode = \"\"\"CREATE TABLE IF NOT EXISTS d_arrivalmode\n",
    "(id INT, mode CHAR(12));\"\"\"\n",
    "\n",
    "create_visatype = \"\"\"CREATE TABLE IF NOT EXISTS d_visatype\n",
    "(id INT, visa_type CHAR(8));\"\"\"\n",
    "\n",
    "create_address = 'CREATE TABLE IF NOT EXISTS d_address (id CHAR(5), name VARCHAR);'\n",
    "\n",
    "create_port = 'CREATE TABLE IF NOT EXISTS d_port (id CHAR(5), name VARCHAR);'\n",
    "\n",
    "create_country = \"\"\"CREATE TABLE IF NOT EXISTS d_country\n",
    "(id INT PRIMARY KEY, name VARCHAR NOT NULL);\"\"\"\n",
    "\n",
    "create_city = \"\"\"CREATE TABLE IF NOT EXISTS d_city\n",
    "(city VARCHAR, state VARCHAR, median_age numeric, male_pop INT, female_pop INT, total_pop INT, num_vets INT,\n",
    "foreign_born INT, avg_household_size FLOAT, state_code CHAR(2), race VARCHAR, count INT);\n",
    "\"\"\"\n",
    "\n",
    "create_state = \"\"\"CREATE TABLE IF NOT EXISTS d_state\n",
    "(state_code CHAR(2) PRIMARY KEY, male_pop INT, female_pop INT, total_pop INT, foreign_born INT);\"\"\"\n",
    "\n",
    "create_date = \"\"\"CREATE TABLE d_date\n",
    "(id INT PRIMARY KEY, year INT NOT NULL, month INT NOT NULL,\n",
    " day INT NOT NULL, day_of_week INT NOT NULL, ymd_dash CHAR(10) NOT NULL,\n",
    " ymd_nodash CHAR(8) NOT NULL, mdy_nodash CHAR(8) NOT NULL);\n",
    "\"\"\"\n",
    "\n",
    "create_immig_fact = \"\"\"CREATE TABLE IF NOT EXISTS fact_immigration \n",
    "(immigration_id INTEGER IDENTITY(0,1) PRIMARY KEY,\n",
    " cicid INT NOT NULL,\n",
    " i94yr INT NOT NULL,\n",
    " i94mon INT NOT NULL,\n",
    " i94cit INT,\n",
    " i94res INT,\n",
    " i94port CHAR(5),\n",
    " arrdate INT,\n",
    " i94mode INT,\n",
    " i94addr CHAR(5),\n",
    " depdate INT,\n",
    " i94bir INT,\n",
    " i94visa INT,\n",
    " count INT,\n",
    " dtadfile VARCHAR,\n",
    " visapost CHAR(5),\n",
    " occup CHAR(5),\n",
    " entdepa CHAR(1),\n",
    " entdepd CHAR(1),\n",
    " entdepu CHAR(1),\n",
    " matflag CHAR(1),\n",
    " biryear INT,\n",
    " dtaddto VARCHAR,\n",
    " gender CHAR(1),\n",
    " insnum VARCHAR,\n",
    " airline CHAR(3),\n",
    " admnum VARCHAR,\n",
    " fltno VARCHAR,\n",
    " visatype CHAR(3)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_temp = \"\"\"CREATE TABLE IF NOT EXISTS d_temperature (\n",
    "    dt date, avg_temp numeric, avg_temp_uncertainty numeric, city varchar,\n",
    "    country varchar, latitude varchar, longitude varchar\n",
    ");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_dim_table_queries = [create_arrivalmode, create_visatype, create_date, create_address, create_port, create_country, create_city, create_state, create_temp]\n",
    "for query in create_dim_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_fact_table_queries = [create_immig_fact]\n",
    "for query in create_fact_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-28 22:38:48,562 INFO  [immig_etl] Populating visa type\n",
      "2020-01-28 22:38:52,323 INFO  [immig_etl] Finished inserting visa type\n"
     ]
    }
   ],
   "source": [
    "# Populate visa type dimension table\n",
    "LOG.info('Populating visa type')\n",
    "insert_visatype = (\"\"\"INSERT INTO d_visatype (id, visa_type) VALUES (1, 'Business'), (2, 'Pleasure'), (3, 'Student');\"\"\")\n",
    "cur.execute(insert_visatype)\n",
    "conn.commit()\n",
    "LOG.info('Finished inserting visa type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-28 22:40:52,286 INFO  [immig_etl] Populating arrival mode dim table\n",
      "2020-01-28 22:40:55,804 INFO  [immig_etl] Finshed populating arrival mode dim table\n"
     ]
    }
   ],
   "source": [
    "# Populate arrival mode dimension table\n",
    "LOG.info('Populating arrival mode dim table')\n",
    "insert_arrivalmode = (\"\"\"INSERT INTO d_arrivalmode (id, mode) VALUES (1, 'Air'), (2, 'Sea'), (3, 'Land'), (9, 'Not reported');\"\"\")\n",
    "cur.execute(insert_arrivalmode)\n",
    "conn.commit\n",
    "LOG.info('Finshed populating arrival mode dim table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-28 22:43:24,340 INFO  [immig_etl] Populating address dim table\n",
      "2020-01-28 22:43:34,361 INFO  [immig_etl] Finished populating address dim table\n"
     ]
    }
   ],
   "source": [
    "# Populate address dimension by reading header file lines and parsing out the address lines.\n",
    "LOG.info('Populating address dim table')\n",
    "insert_address = 'INSERT INTO d_address (id, name) VALUES (%s, %s);'\n",
    "\n",
    "with open('data/I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    desc_lines = f.readlines()\n",
    "\n",
    "address_lines = desc_lines[981:1036]\n",
    "regex = re.compile(r\"^\\s*'(?P<id>..)'\\s*=\\s*'(?P<name>.+)'.*$\")\n",
    "addresses = [regex.match(line) for line in address_lines]\n",
    "adds = {a.group('id'): a.group('name') for a in addresses}\n",
    "assert len(adds) == len(address_lines)\n",
    "\n",
    "for it in sorted(adds.items()):\n",
    "    cur.execute(insert_address, list(it))\n",
    "LOG.info('Finished populating address dim table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-28 22:59:19,966 INFO  [immig_etl] Populating arrival port dim table\n",
      "2020-01-28 23:00:12,727 INFO  [immig_etl] Finished populating arrival port dim table\n"
     ]
    }
   ],
   "source": [
    "# Populate port dimension table\n",
    "LOG.info('Populating arrival port dim table')\n",
    "query = 'INSERT INTO d_port (id, name) VALUES (%s, %s);'\n",
    "\n",
    "with open('data/I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    desc_lines = f.readlines()\n",
    "\n",
    "port_lines = desc_lines[302:962]        \n",
    "regex = re.compile(r\"^\\s*'(?P<id>...?)'\\s*=\\s*'(?P<name>.+)'.*$\")\n",
    "ports = [regex.match(line) for line in port_lines]\n",
    "allports = {p.group('id'): p.group('name').strip() for p in ports}\n",
    "assert len(allports) == len(port_lines)\n",
    "\n",
    "for it in sorted(allports.items()):\n",
    "    cur.execute(query, list(it))\n",
    "LOG.info('Finished populating arrival port dim table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-28 23:51:06,269 INFO  [immig_etl] Populating city dim table\n",
      "2020-01-28 23:57:04,979 INFO  [immig_etl] Finished populating city dim table\n"
     ]
    }
   ],
   "source": [
    "# Populate city dimension table\n",
    "LOG.info('Populating city dim table')\n",
    "insert_city = (\"\"\"INSERT INTO d_city\n",
    "(city, state, median_age, male_pop, female_pop, total_pop, num_vets, foreign_born, avg_household_size, state_code, race, count)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\"\")\n",
    "\n",
    "city_df = pd.read_csv('data/us-cities-demographics.csv', delimiter=';')\n",
    "for _, row in city_df.iterrows():\n",
    "    cur.execute(insert_city, [city if pd.notna(city) else None for city in row])\n",
    "\n",
    "LOG.info('Finished populating city dim table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-28 23:57:47,114 INFO  [immig_etl] Populating state dim table\n",
      "2020-01-28 23:57:55,428 INFO  [immig_etl] Finished populating state dim table\n"
     ]
    }
   ],
   "source": [
    "# Populate state dimension table.\n",
    "LOG.info('Populating state dim table')\n",
    "insert_state = \"\"\"INSERT INTO d_state (state_code, male_pop, female_pop, total_pop, foreign_born)\n",
    "                  VALUES (%s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "city_df = pd.read_csv('data/us-cities-demographics.csv', delimiter=';')\n",
    "state_df = city_df[['State Code', 'Male Population', 'Female Population', 'Total Population', 'Foreign-born']].groupby(['State Code']).sum().reset_index()\n",
    "\n",
    "for _, row in state_df.iterrows():\n",
    "    cur.execute(insert_state, list(row))\n",
    "    conn.commit\n",
    "LOG.info('Finished populating state dim table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-29 00:02:25,604 INFO  [immig_etl] Populating country dim table\n",
      "2020-01-29 00:02:54,408 INFO  [immig_etl] Finished populating country dim table\n"
     ]
    }
   ],
   "source": [
    "#Populating country dimension table\n",
    "LOG.info('Populating country dim table')\n",
    "query = \"\"\"INSERT INTO d_country (id, name) VALUES (%s, %s);\"\"\"\n",
    "\n",
    "with open('data/I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    desc_lines = f.readlines()\n",
    "\n",
    "country_lines = desc_lines[9:298]\n",
    "regex = re.compile(r\"^\\s*(?P<id>\\d+)\\s*=\\s*'(?P<name>.+)'.*$\")\n",
    "countries = [regex.match(l) for l in country_lines]\n",
    "allcountries = {int(c.group('id')): c.group('name') for c in countries}\n",
    "assert len(allcountries) == len(country_lines)\n",
    "\n",
    "for it in sorted(allcountries.items()):\n",
    "    cur.execute(query, list(it))\n",
    "\n",
    "LOG.info('Finished populating country dim table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-29 03:55:39,512 INFO  [immig_etl] Populating temperature dim table\n"
     ]
    }
   ],
   "source": [
    "#Populating temperature dimension table.\n",
    "LOG.info('Populating temperature dim table')\n",
    "insert_temp = (\"\"\"INSERT INTO d_temperature(dt, avg_temp, avg_temp_uncertainty, city, country, latitude, longitude)\n",
    "                 VALUES (%s, %s, %s, %s, %s, %s, %s);\"\"\")\n",
    "\n",
    "temperature_df = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "for _, row in temperature_df.iterrows():\n",
    "    cur.execute(insert_temp, [t if pd.notna(t) else None for t in row])\n",
    "    conn.commit\n",
    "LOG.info('Finished populating temperature dim table')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Populate fact_immigration table from etl.py. This runs for a very long time. Easier to run from python script.\n",
    "# June data has extra columns that need not be imported.\n",
    "\n",
    "   engine=create_engine(conn)\n",
    "    immig_files = ['../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
    "                           '../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat']\n",
    "    for filename in immig_files:\n",
    "        LOG.info(f'reading {filename}')\n",
    "        iterr = pd.read_sas(filename, 'sas7bdat', encoding='ISO-8859-1', chunksize=500000)\n",
    "        \n",
    "        for sub_df in iterr:\n",
    "            try:\n",
    "                immigration_df = pd.concat([immigration_df, sub_df], join='inner')\n",
    "            except NameError:\n",
    "                immigration_df = sub_df\n",
    "\n",
    "\n",
    "        if 'may16' in filename:\n",
    "            continue # June has the columns to be deleted.\n",
    "        else:\n",
    "            immigration_df.to_sql(\n",
    "                'fact_immigration',\n",
    "                engine,\n",
    "                if_exists='append',\n",
    "                chunksize=50000,\n",
    "                index=False\n",
    "            )\n",
    "            logger.info('done inserting')\n",
    "            del immigration_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Confirm dimension tables are populated and have the expected rows.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Perform data quality checks on dimension tables\n",
    "dim_tables = ['d_arrivalmode', 'd_visatype', 'd_address', 'd_port', 'd_country', 'd_city', 'd_state', 'd_temperature']\n",
    "for table in dim_tables:\n",
    "    query = \"SELECT COUNT(*) FROM \" + table\n",
    "    cur.execute(query)\n",
    "    conn.commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Confirm the fact_immigration table is populated"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "select count(distinct immigration_id), i94mon\n",
    "from fact_immigration\n",
    "group by i94mon\n",
    "order by i94mon;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Birth year must be reasonable. Human life expectancy must be limited to 125."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "UPDATE fact_immigration\n",
    "SET i94bir = NULL\n",
    "WHERE i94bir < 0 or i94bir > 125;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "fact_immigration - Fact table. Each entry represents an immigration event.\n",
    "\n",
    "immigration_id: primary key\n",
    "cicid: unique key within a month\n",
    "i94yr: 4 digit year\n",
    "i94mon: numeric month, 1-12\n",
    "i94cit: immigrant's country of origin, foreign key to dim_country\n",
    "i94res: immigrant's country of residence. foreign key to dim_country\n",
    "i94port: port of entry; foreign key to dim_port\n",
    "arrdate: arrival date of immigrant\n",
    "i94mode: arrival mode; foreign key to dim_arrival_mode\n",
    "i94addr: immigrant's address; foreign key to dim_address\n",
    "depdate: departure date\n",
    "i94bir: immigrant's age in years\n",
    "i94visa: visa , foreign key to dim_visa_type\n",
    "count: used for summary statistics; always 1 (for easy adding)\n",
    "dtadfile: dates YYYYMMDD\n",
    "visapost: visa issual post\n",
    "occup: immigrants occupation\n",
    "entdepa: arrival code\n",
    "entdepd: departure code\n",
    "entdepu: update code\n",
    "matflag: M if the arrival and departure records match\n",
    "biryear: birth year\n",
    "dtaddto: MMDDYYYY end date of admission\n",
    "gender: mostly M and F, also X and U\n",
    "insnum: Immigration and Naturalization Services number\n",
    "airline: Airline of entry for immigrant\n",
    "admnum: admission number\n",
    "fltno: flight number of immigrant\n",
    "visatype: short visa codes like WT, B2, WB, etc.\n",
    "\n",
    "================================================\n",
    "d_city -  Population statistics on cities in the US. Each entry is city/state/race.\n",
    "\n",
    "city: city's name\n",
    "state: state of city\n",
    "median_age: median age of city\n",
    "male_pop: population of men in the city\n",
    "female_pop: population of women in the city\n",
    "total_pop: population of all people in the city\n",
    "num_vets: population of veterans in the city\n",
    "foreign_born: population of foreign-born people in the city\n",
    "avg_household_size: average household size\n",
    "state_code: State abbreviation\n",
    "race: White, Hispanic or Latino, Asian, Black or African-American, or American Indian and Alaska Native\n",
    "count: population of people of that race in the city\n",
    "\n",
    "================================================\n",
    "d_state - Aggregated statistics from dim_city by state\n",
    "\n",
    "state_code: two-letter abbreviation for state\n",
    "male_pop: population of men in the state\n",
    "female_pop: population of women in the state\n",
    "total_pop: population of all people in the state\n",
    "foreign_born: population of foreign-born people in the state\n",
    "\n",
    "================================================\n",
    "d_country - A list of countries and their codes \n",
    "\n",
    "code: id\n",
    "name: usually a name of a country. There are many that start with INVALID: as well as several different No Country Code([code]) values\n",
    "\n",
    "================================================\n",
    "d_address - immigrants  address\n",
    "\n",
    "id: mostly two-letter abbrev for states. There's DC, GU (Guam), and 99 (All Other Codes) as well\n",
    "name: name of state, region, etc.\n",
    "\n",
    "================================================\n",
    "d_port - A list of the ports of arrival\n",
    "\n",
    "id: id of port\n",
    "name: the name of the port; there are some No PORT Code ([code]) values too\n",
    "\n",
    "================================================\n",
    "d_date - Dates\n",
    "\n",
    "code: the CIC code for date where 20454 is 1/1/2016\n",
    "year: four-digit year\n",
    "month: month; 1-12\n",
    "day: day; 1-31\n",
    "day_of_week: 0-7\n",
    "ymd_dash: date formatted as YYYY-MM-DD\n",
    "ymd_nodash: date formatted as YYYYMMDD\n",
    "mdy_noash: date formatted as MMDDYYYY\n",
    "\n",
    "================================================\n",
    "d_arrival_mode - Arrival mode. Foreign key to fact_immigration.i94mode\n",
    "\n",
    "id: 1, 2, 3, or 9\n",
    "mode: Air, Sea, Land, or Not reported, respectively\n",
    "\n",
    "================================================\n",
    "d_visa_type - Visa type. Foreigy key to fact_immigration.i94visa\n",
    "\n",
    "id: 1, 2, or 3\n",
    "visa_type: Business, Pleasure, or Student, respectively\n",
    "================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Summary\n",
    "\n",
    "### Tools and Technologies\n",
    "* Tools used are Amazom Redshift database. I used a relational database to make querying my fact and dimensional tables easy and gain analytics on immigration patterns. Immigration data was well structured, that I was able to use a relational Db.\n",
    "* Chose Redshift because it is fast, scalable and highly performant.\n",
    "* ETL itself was written using Python, and pandas. This allowed me to explore the data, understand and come up with a _star schema_ with immigration fact table and several dimension tables.\n",
    "* By creating city, state, country dimension tables, we are able to see the states immigrants are immigrating to and it also helps us understand the demographics of the cities and states they are headed to.\n",
    "\n",
    "### Update frequency\n",
    "* fact_immigration must be updated monthly when each new dataset is available.\n",
    "* Dim tables can be dropped and recreated completely whenever fact_immigration is updated.\n",
    "* dim_city can be simply updated whenever city's metadata changes or inserted when a city is not found.\n",
    "* dim_state can be simply updated whenever city's metadata changes. this is an aggregate.\n",
    "\n",
    "### If data was increased 100x\n",
    "* If data was increased 100x, I would use Redshift spectrum. Here SQL queries can be performed on data that is stored in Amazon S3 buckets.\n",
    "* This can save time and money because it eliminates the need to move data from a storage service to a database, and instead directly queries data inside an S3 bucket. \n",
    "* Redshift Spectrum also expands the scope of a given query because it extends beyond a user's existing Redshift data warehouse nodes and into large volumes of unstructured S3 data lakes.\n",
    "\n",
    "REF: \n",
    "* https://searchaws.techtarget.com/definition/Amazon-Redshift-Spectrum\n",
    "* https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\n",
    "\n",
    "### If dashboard must be updated by 7am daily\n",
    "* I would build a concurrent data orchestration pipeline using Amazon EMR and Apache Spark. An Airflow DAG that has an S3 sensor could read data as soon as it arrives, parse the data and land it in S3 location immediately. Our Reshift spectrum detailed above could read the data immediately to populate the dashboard.\n",
    "\n",
    "REF: \n",
    "* https://aws.amazon.com/blogs/big-data/build-a-concurrent-data-orchestration-pipeline-using-amazon-emr-and-apache-livy/\n",
    "* https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\n",
    "\n",
    "### If database needed to be accessed by 100+ people\n",
    "* AWS Redshift spectrum with date partitioned data should perform well for multiple users.\n",
    "* We could have data replicated to different nodes, if our users are global we could have replication nodes closer to user's location.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
